# OpenClaw Automated Backup Strategy
## Complete Plan for Every 2 Hours

**Date**: February 4, 2026  
**Status**: PLANNING PHASE (NO EXECUTION YET)  
**For**: Glenn's Complete System Backup

---

## ğŸ“‹ EXECUTIVE SUMMARY

Glenn wants **automated backups of EVERYTHING** every 2 hours. This includes:
- âœ… All OpenClaw configuration & settings
- âœ… Workspace (all work from day 1)
- âœ… ERP project (backend, frontend, docs)
- âœ… Databases (when running)
- âœ… Configuration files (.env, credentials)
- âœ… Git repositories
- âœ… Agent memory & sessions

**Key constraint**: Active VS Code / Claude Code session running (started 05:33 UTC) - MUST NOT DISTURB

---

## ğŸ” PART 1: WHAT'S RUNNING (Current System State)

### Active Processes
```
RUNNING:
âœ“ VS Code Server (Claude Code IDE) - PIDs 57340, 57476, 57495, 57511, 57533, 57560
  â””â”€ Started: 05:33 UTC Feb 4
  â””â”€ Status: ACTIVE - coding session in progress
  â””â”€ Impact: MUST USE SAFE BACKUP (read-only, non-blocking)

âœ“ Python HTTP Server - PID 30027
  â””â”€ Port: 8000
  â””â”€ Running dashboard

NOT RUNNING:
âœ— Docker containers (PostgreSQL, Redis, Celery)
âœ— System cron (using OpenClaw's custom cron system instead)
```

### Safe Backup Window
- âœ… Can backup while VS Code runs (read-only operations)
- âœ… Can backup workspace (not being edited in VS Code right now)
- âœ… Should avoid if heavy coding is happening
- âš ï¸ Must use `tar`/`rsync` (safe, non-blocking copy methods)

---

## ğŸ—‚ï¸ PART 2: WHAT NEEDS TO BE BACKED UP

### A. OpenClaw Core Configuration (7.2 MB total)

```
/home/ubuntu/.openclaw/
â”œâ”€â”€ openclaw.json                 â† Main config (API keys, settings)
â”œâ”€â”€ openclaw.json.backup          â† Previous backup
â”œâ”€â”€ agents/main/
â”‚   â”œâ”€â”€ agent/                    â† Agent settings
â”‚   â””â”€â”€ sessions/                 â† Session data
â”œâ”€â”€ credentials/                  â† Sensitive (Telegram, API auth)
â”œâ”€â”€ identity/                     â† Agent identity files
â”œâ”€â”€ devices/                      â† Paired device configs
â”œâ”€â”€ telegram/                     â† Telegram integration
â”œâ”€â”€ cron/                         â† Scheduled jobs
â”‚   â”œâ”€â”€ jobs.json                 â† Cron jobs config
â”‚   â””â”€â”€ runs/                     â† Job execution history
â”œâ”€â”€ logs/                         â† System logs
â”œâ”€â”€ canvas/                       â† Canvas UI configs
â””â”€â”€ subagents/                    â† Subagent sessions
```

**Sensitive Files** âš ï¸
- `credentials/telegram-*`
- `openclaw.json` (contains API keys)

### B. Workspace (1.3 MB total)

```
/home/ubuntu/.openclaw/workspace/
â”œâ”€â”€ *.md                          â† Documents (AGENTS.md, SOUL.md, USER.md, etc.)
â”œâ”€â”€ .git/                         â† Main workspace git repo
â”œâ”€â”€ memory/                       â† Daily & long-term memory files
â”‚   â”œâ”€â”€ 2026-02-01.md
â”‚   â”œâ”€â”€ 2026-02-02.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ai-erp/                       â† MAIN PROJECT
â”‚   â”œâ”€â”€ .git/                     â† ERP git repo
â”‚   â”œâ”€â”€ docker-compose.yml        â† Service definitions
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ app/                  â† 12 database models, FastAPI
â”‚   â”‚   â”œâ”€â”€ alembic/              â† Database migrations
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ .env.example
â”‚   â”œâ”€â”€ frontend/                 â† React/Next.js
â”‚   â”œâ”€â”€ docs/                     â† Documentation
â”‚   â””â”€â”€ infrastructure/           â† Terraform, deployment configs
â””â”€â”€ [other project files]
```

**Key Files to Preserve**:
- All `.md` documentation
- Git history (both repos)
- ERP code structure
- Docker Compose definition
- Migration files

### C. Databases (When Running)

```
PostgreSQL:
â”œâ”€â”€ Database: ai_erp
â”‚   â”œâ”€â”€ 12 tables (Tenant, Client, User, Vendor, Invoice, GL, etc.)
â”‚   â””â”€â”€ Data & schemas
â””â”€â”€ Running on: localhost:5432 (Docker container)

Redis:
â”œâ”€â”€ Cache data (6379)
â”œâ”€â”€ Celery broker (6379/1)
â”œâ”€â”€ Celery results (6379/2)
â””â”€â”€ Running on: localhost:6379 (Docker container)

Git Repositories:
â”œâ”€â”€ /home/ubuntu/.openclaw/workspace/.git/
â””â”€â”€ /home/ubuntu/.openclaw/workspace/ai-erp/.git/
```

---

## ğŸ“Š PART 3: BACKUP SCOPE & SIZING

### Current System Size
```
/home/ubuntu/.openclaw/           7.2 MB
/home/ubuntu/.openclaw/workspace/ 1.3 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total (without DB)                8.5 MB

When Docker running:
+ PostgreSQL data volume          ~50-200 MB (starts small, grows)
+ Redis data                      ~5-50 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Estimated total with DB:          ~60-260 MB per backup

Backup frequency:                 Every 2 hours
Backups per day:                  12 backups
Daily backup size:                ~720 MB - 3.1 GB
Storage for 7 days:               ~5 GB - 22 GB
Storage for 30 days:              ~22 GB - 93 GB
```

### Disk Space Status
```
Current usage: 5.1 GB / 6.8 GB (76% full)
Available: 1.7 GB FREE

âš ï¸ CRITICAL: Only 1.7 GB free space!
Problem: Keeping 7 days of backups locally would need 5-22 GB
Solution: Use remote storage (S3, external drive, or rotate aggressively)
```

---

## ğŸ”„ PART 4: HOW CRONJOBS WORK IN OPENCLAW

OpenClaw does **NOT** use system cron (`/etc/cron.d`). Instead, it uses:

### OpenClaw's Custom Cron System

**Location**: `/home/ubuntu/.openclaw/cron/`

**Files**:
```
jobs.json          â† Defines all scheduled tasks (JSON)
jobs.json.bak      â† Automatic backups of job config
runs/               â† Directory with execution logs for each job
  â”œâ”€â”€ [job-id].jsonl  â† Execution history
```

**Job Structure** (from jobs.json):
```json
{
  "id": "unique-job-id",
  "agentId": "main",
  "name": "Descriptive job name",
  "enabled": true,
  "schedule": {
    "kind": "interval",        // or "at" for one-time
    "intervalMs": 7200000      // 2 hours in milliseconds
  },
  "sessionTarget": "main",
  "wakeMode": "next-heartbeat" // or "immediate"
  "payload": {
    "kind": "systemEvent",
    "text": "Command or action to run"
  }
}
```

### Schedule Types in OpenClaw

| Type | Format | Example |
|------|--------|---------|
| **interval** | Every N ms | `"intervalMs": 7200000` (2 hours) |
| **at** | One-time at timestamp | `"atMs": 1738504800000` |
| **cron** | Traditional cron (if supported) | `"cron": "0 */2 * * *"` |

### How It Works

1. **OpenClaw reads** `jobs.json` on startup
2. **Schedules jobs** using its internal job runner (NOT system cron)
3. **Triggers jobs** based on schedule
4. **Logs execution** to `runs/[job-id].jsonl`
5. **Reports status** (ok, failed, etc.)

### Current Jobs in System

```
âœ“ "Ask Glenn questions"           - One-time reminder (disabled)
âœ“ "Conference deep-dive"          - One-time reminder (disabled)
âœ“ "Gmail Workspace Reminder"      - One-time reminder (disabled)
âœ“ "Webhook-vurdering reminder"    - One-time reminder (disabled)
```

All are **disabled**. No active scheduled tasks currently.

---

## ğŸ›¡ï¸ PART 5: BACKUP STRATEGY DESIGN

### Storage Options (Ranked by Practicality)

| Option | Pros | Cons | Cost | Recommendation |
|--------|------|------|------|---|
| **AWS S3** | Unlimited, cheap, reliable, versioning | Need AWS account, network dependency | ~$0.50/month for 250GB | â­ BEST FOR PROD |
| **External USB/NAS** | Local, fast, reliable | Single point of failure, not cloud | ~$100-500 one-time | â­ BEST FOR INITIAL |
| **GitHub Private Repo** | Version control, encrypted, free tier | Size limits (encrypted tarballs work), slower | FREE (with limits) | âœ“ GOOD FOR REPO-ONLY |
| **Local rotation** | No external deps, no costs | Only 1.7GB space available NOW (problem!) | $0 | âœ— NOT FEASIBLE |
| **Backblaze B2** | Cheap cloud storage | Less integrated than S3 | ~$6/month for 1TB | âœ“ GOOD |

### Recommended Strategy: Tiered Backup Approach

```
TIER 1: Git Repositories (Most Important)
â”œâ”€â”€ Automatic git push to GitHub (every backup)
â”œâ”€â”€ Preserves: /home/ubuntu/.openclaw/workspace/.git
â”œâ”€â”€ Preserves: /home/ubuntu/.openclaw/workspace/ai-erp/.git
â”œâ”€â”€ Risk: Code loss if GitHub compromised
â””â”€â”€ Safety: Public GitHub Private + SSH auth

TIER 2: Configuration & Settings (Critical)
â”œâ”€â”€ Target: /home/ubuntu/.openclaw/ (excluding sensitive)
â”œâ”€â”€ Method: Encrypted tarball to S3 or external storage
â”œâ”€â”€ Frequency: Every 2 hours
â”œâ”€â”€ Retention: 7-day rolling window (84 backups)
â””â”€â”€ Safety: Encryption + versioning

TIER 3: Database Snapshots (When Running)
â”œâ”€â”€ Target: PostgreSQL & Redis data volumes
â”œâ”€â”€ Method: docker exec + dump to tarball
â”œâ”€â”€ Frequency: Every 2 hours (only if containers running)
â”œâ”€â”€ Retention: 3-day window
â””â”€â”€ Safety: Point-in-time recovery

TIER 4: Full System Snapshot (Weekly)
â”œâ”€â”€ Target: Complete /home/ubuntu/.openclaw/
â”œâ”€â”€ Method: Full encrypted archive
â”œâ”€â”€ Frequency: Once per week (Sunday 00:00)
â”œâ”€â”€ Retention: 4-week rotation
â””â”€â”€ Safety: Complete disaster recovery
```

---

## ğŸš€ PART 6: IMPLEMENTATION WITHOUT DISRUPTION

### Step 1: Create Backup Script (Safe for Active Sessions)

**File**: `/home/ubuntu/.openclaw/scripts/backup-all.sh`

**Design Principles**:
- âœ… Read-only operations (no locks, no interruptions)
- âœ… Uses `tar` with `--exclude` to skip active files
- âœ… Compresses to save space
- âœ… Encrypts sensitive data
- âœ… Validates Git repos
- âœ… Logs all activity
- âœ… Handles DB backups gracefully (if running)
- âœ… Checks disk space before starting

**Pseudo-code** (NOT EXECUTED YET):
```bash
#!/bin/bash
# backup-all.sh - Run every 2 hours

set -e
BACKUP_DIR="/mnt/backups"  # External storage
LOG_FILE="/var/log/openclaw-backup.log"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# 1. Check space
check_disk_space()

# 2. Backup Tier 1: Git repos
push_to_github()
tar_git_history()

# 3. Backup Tier 2: Configuration
tar_openclaw_config()    # Compress /home/ubuntu/.openclaw/
tar_workspace_docs()     # Compress /home/ubuntu/.openclaw/workspace/*.md

# 4. Backup Tier 3: Database (if running)
if docker ps | grep -q postgres; then
    docker exec ai-erp-postgres pg_dump -U erp_user ai_erp | \
    gzip > $BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz
fi

# 5. Encrypt sensitive backups
encrypt_with_gpg()

# 6. Upload to S3 (if configured)
aws s3 sync $BACKUP_DIR s3://backup-bucket/

# 7. Log status
echo "Backup completed: $BACKUP_SIZE" >> $LOG_FILE
```

### Step 2: Register Job in OpenClaw Cron

**Add to** `/home/ubuntu/.openclaw/cron/jobs.json`:

```json
{
  "id": "backup-every-2-hours",
  "agentId": "main",
  "name": "Automated Full System Backup (Every 2 Hours)",
  "enabled": false,  // START AS FALSE until tested
  "createdAtMs": 1738580000000,
  "updatedAtMs": 1738580000000,
  "schedule": {
    "kind": "interval",
    "intervalMs": 7200000  // 2 hours = 2 * 60 * 60 * 1000
  },
  "sessionTarget": "system",  // Run as system event, not in main session
  "wakeMode": "immediate",     // Don't wait for heartbeat, run immediately
  "payload": {
    "kind": "systemEvent",
    "command": "/home/ubuntu/.openclaw/scripts/backup-all.sh"
  },
  "state": {
    "lastRunAtMs": null,
    "lastStatus": "pending",
    "lastDurationMs": 0
  }
}
```

### Step 3: How the Job Runs

```
â”Œâ”€ OpenClaw Main Process
â”‚
â”œâ”€ Reads jobs.json at startup
â”‚
â”œâ”€ Registers "backup-every-2-hours" job
â”‚  â””â”€ Interval: 7200000 ms (2 hours)
â”‚
â”œâ”€ Job Scheduler tracks time elapsed
â”‚  â””â”€ Every 2 hours, triggers job
â”‚
â”œâ”€ Executes /home/ubuntu/.openclaw/scripts/backup-all.sh
â”‚  â”œâ”€ SAFE: Runs as background process
â”‚  â”œâ”€ SAFE: Doesn't block VS Code
â”‚  â”œâ”€ SAFE: Read-only file operations
â”‚  â””â”€ LOGS: Output to /var/log/openclaw-backup.log
â”‚
â”œâ”€ Logs execution to runs/backup-every-2-hours.jsonl
â”‚  â””â”€ Records: start time, duration, status, errors
â”‚
â””â”€ Repeats every 2 hours indefinitely
```

### Step 4: Storage Setup Options

**Option A: External USB Drive** (Recommended for testing)
```bash
# Mount external drive
sudo mount /dev/sdb1 /mnt/backups

# Check space
df -h /mnt/backups

# Persist in /etc/fstab for auto-mount
```

**Option B: AWS S3** (Recommended for production)
```bash
# Create S3 bucket
aws s3 mb s3://glenn-openclaw-backups

# Configure credentials
aws configure

# Script uploads to S3 automatically
aws s3 sync /mnt/backups s3://glenn-openclaw-backups/
```

**Option C: Network Storage (NAS)**
```bash
# Mount NAS
sudo mount -t nfs 192.168.1.100:/backups /mnt/backups

# Same as above, S3 or manual upload
```

---

## ğŸ“‹ PART 7: DETAILED BACKUP MANIFEST

### What Gets Backed Up Every 2 Hours

```
TIER 1: Git Repositories (Pushed to GitHub every 2 hours)
âœ“ /home/ubuntu/.openclaw/workspace/.git/
  â””â”€ Preserves: Agent workspace history, all docs
âœ“ /home/ubuntu/.openclaw/workspace/ai-erp/.git/
  â””â”€ Preserves: ERP backend, frontend, infrastructure code

TIER 2: Configuration & Settings (Encrypted tarball)
âœ“ /home/ubuntu/.openclaw/openclaw.json          (Main config)
âœ“ /home/ubuntu/.openclaw/agents/                (Agent settings)
âœ“ /home/ubuntu/.openclaw/cron/                  (Job definitions)
âœ“ /home/ubuntu/.openclaw/identity/              (Identity)
âœ“ /home/ubuntu/.openclaw/logs/                  (System logs)
âœ“ /home/ubuntu/.openclaw/canvas/                (Canvas configs)
âš ï¸ /home/ubuntu/.openclaw/credentials/          (SENSITIVE - encrypt!)
âš ï¸ /home/ubuntu/.openclaw/telegram/             (SENSITIVE - encrypt!)

TIER 2b: Workspace Documentation
âœ“ /home/ubuntu/.openclaw/workspace/*.md         (All docs)
âœ“ /home/ubuntu/.openclaw/workspace/memory/      (Agent memory)
âœ“ /home/ubuntu/.openclaw/workspace/ai-erp/*.md (ERP docs)

TIER 3: Database (If Docker containers running)
âœ“ PostgreSQL database dump (ai_erp)
  â””â”€ 12 tables with all data
âœ“ Redis dump
  â””â”€ Cache & Celery state

TIER 4: Excluded Files (Don't backup)
âœ— /home/ubuntu/.openclaw/workspace/.git/objects/  (Git objects already in repo)
âœ— /home/ubuntu/.vscode-server/                    (Active IDE - skip)
âœ— /home/ubuntu/.cache/                            (Cache, not data)
âœ— /home/ubuntu/.npm/                              (Package cache)
âœ— node_modules/                                   (Recreated from package.json)
âœ— __pycache__/                                    (Python cache)
âœ— *.pyc, *.so                                     (Compiled files)

```

### Files NOT to Include (Why?)
- **VS Code files** (.vscode-server): Active session, regenerated
- **node_modules**: Recreated from package.json on restore
- **Python cache**: Recreated automatically
- **Git objects**: Already preserved in .git directory
- **Logs** (optional): Can exclude if saving space

---

## âš™ï¸ PART 8: CRON SYSTEM MECHANICS (Deep Dive)

### How OpenClaw's Cron Works

OpenClaw **does not use system crontab**. Instead:

1. **Daemon reads** `/home/ubuntu/.openclaw/cron/jobs.json`
2. **In-memory scheduler** tracks each job's next execution time
3. **Wakes at intervals** to check if any jobs need running
4. **Executes payload** (shell command, system event, or script)
5. **Records result** in `runs/[job-id].jsonl`
6. **Calculates next run** based on schedule

### Scheduler Accuracy

- **Â±30 seconds**: Normal variance (good enough for 2-hour backups)
- **+/- up to 5 minutes**: If system under load
- **NOT guaranteed**: If OpenClaw process crashes, jobs don't run
- **Persists**: Jobs.json survives restarts; next run calculated on restart

### Advantages Over System Cron

| Feature | System Cron | OpenClaw Cron |
|---------|------------|---|
| **When active** | Always running | Only when OpenClaw daemon running |
| **Status tracking** | Logs to syslog (hard to parse) | JSON logs in runs/ directory |
| **Dynamic updates** | Requires crontab edit + service restart | Edit jobs.json, reread on next heartbeat |
| **Permissions** | Requires sudo for system-level tasks | Runs as OpenClaw user (safer) |
| **Logging** | Scattered system logs | Centralized in runs/*.jsonl |
| **Chaining** | Complex (need wrapper scripts) | Native support via payload |

### Backup Job Lifecycle

```
TIME    EVENT                           STATE
â”€â”€â”€â”€    â”€â”€â”€â”€â”€                           â”€â”€â”€â”€
00:00   Job created (enabled: false)    IDLE
        â””â”€ Waiting for human approval

00:00   Human enables job               SCHEDULED
        â””â”€ "enabled": true
        â””â”€ Next run: NOW (if past)

00:00   First execution triggers        RUNNING
        â”œâ”€ Command: /backup-all.sh
        â”œâ”€ Duration: ~2-5 minutes
        â””â”€ Result: success/failed

02:00   Second execution                RUNNING
        â””â”€ Same process repeats

04:00   Third execution                 RUNNING
...     ...                             ...

Every 2 hours, forever, until:
- Job is disabled
- OpenClaw daemon stops
- System reboots (resumes on daemon restart)
```

### Execution Log Format

**File**: `/home/ubuntu/.openclaw/cron/runs/backup-every-2-hours.jsonl`

```json
{"timestamp":"2026-02-04T06:00:00Z","status":"started","jobId":"backup-every-2-hours"}
{"timestamp":"2026-02-04T06:02:15Z","status":"completed","jobId":"backup-every-2-hours","duration":"135s","backupSize":"850MB","uploaded":true}
{"timestamp":"2026-02-04T08:00:00Z","status":"started","jobId":"backup-every-2-hours"}
{"timestamp":"2026-02-04T08:02:45Z","status":"completed","jobId":"backup-every-2-hours","duration":"165s","backupSize":"890MB","uploaded":true}
{"timestamp":"2026-02-04T10:00:00Z","status":"failed","jobId":"backup-every-2-hours","error":"Disk full: 0.2GB free","retry":"yes"}
```

---

## ğŸ” PART 9: SECURITY CONSIDERATIONS

### Sensitive Data in Backups

âš ï¸ **These files contain secrets**:

```
âŒ openclaw.json
   â”œâ”€ Anthropic API key
   â”œâ”€ Brave Search API key
   â”œâ”€ Telegram bot token
   â””â”€ Gateway auth token

âŒ credentials/telegram-*.json
   â””â”€ Telegram auth tokens

âŒ .env files (in ai-erp backend)
   â”œâ”€ DATABASE_URL (password)
   â”œâ”€ REDIS_URL
   â””â”€ API keys

âŒ .ssh/
   â”œâ”€ Private keys (DO NOT BACKUP)
   â””â”€ Known hosts
```

### Encryption Strategy

**Recommended**: GPG Encryption

```bash
# Encrypt sensitive files before uploading
tar czf openclaw-backup.tar.gz \
    /home/ubuntu/.openclaw/openclaw.json \
    /home/ubuntu/.openclaw/credentials/ \
    /home/ubuntu/.openclaw/agents/

# Encrypt with GPG
gpg --symmetric --cipher-algo AES256 \
    openclaw-backup.tar.gz
# â†’ openclaw-backup.tar.gz.gpg (password protected)

# Upload encrypted file (safe!)
aws s3 cp openclaw-backup.tar.gz.gpg s3://backups/

# Decrypt on restore
gpg --decrypt openclaw-backup.tar.gz.gpg > restored.tar.gz
tar xzf restored.tar.gz
```

### Restore Safety

- âœ… Only decrypt when restoring (not stored in plain text)
- âœ… Use strong passphrase (20+ chars, mixed)
- âœ… Keep passphrase separate from backups
- âœ… Test restore process regularly

---

## â±ï¸ PART 10: TIMELINE & RETENTION POLICY

### Backup Schedule

```
Every 2 hours automatically (via OpenClaw cron)

Time (UTC)   Backup #   Storage Path
â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
00:00        Backup 1   backups/2026-02-04_00.tar.gz.gpg
02:00        Backup 2   backups/2026-02-04_02.tar.gz.gpg
04:00        Backup 3   backups/2026-02-04_04.tar.gz.gpg
06:00        Backup 4   backups/2026-02-04_06.tar.gz.gpg
...
22:00        Backup 12  backups/2026-02-04_22.tar.gz.gpg

Next day:
00:00        Backup 13  backups/2026-02-05_00.tar.gz.gpg
...
```

### Retention Policy

```
TIER 1: Git Backups (Unlimited)
â”œâ”€ GitHub private repo
â”œâ”€ Retention: Forever
â””â”€ Risk: GitHub compromise (very low)

TIER 2: Rolling Window (7 days)
â”œâ”€ Last 7 days Ã— 12 backups/day = 84 backups
â”œâ”€ Storage: ~7 GB (at 100MB/backup average)
â”œâ”€ Older backups: Deleted automatically
â””â”€ Use case: Restore recent changes

TIER 3: Weekly Snapshots (4 weeks)
â”œâ”€ Every Sunday at 00:00 UTC
â”œâ”€ Storage: ~400 MB (4 weekly backups)
â”œâ”€ Older backups: Deleted after 4 weeks
â””â”€ Use case: Disaster recovery, monthly audits

TIER 4: Monthly Archive (12 months - optional)
â”œâ”€ First Sunday of each month
â”œâ”€ Storage: ~100 MB (1 per month)
â”œâ”€ Retention: 12 months
â””â”€ Use case: Regulatory compliance, year-over-year comparison
```

### Cleanup Script

**File**: `/home/ubuntu/.openclaw/scripts/cleanup-old-backups.sh`

```bash
#!/bin/bash
# Run daily (after backups complete)

BACKUP_DIR="/mnt/backups"
DAYS_TO_KEEP=7

# Delete backups older than 7 days
find $BACKUP_DIR -name "*.tar.gz.gpg" -mtime +$DAYS_TO_KEEP -delete

# Keep at least 4 weekly backups
ls -1t $BACKUP_DIR/weekly_* | tail -n +5 | xargs rm -f

echo "Cleanup completed at $(date)"
```

---

## ğŸ“ˆ PART 11: MONITORING & ALERTING

### What to Monitor

```
âœ“ Backup completion status
  â””â”€ Every job should complete in 2-5 minutes

âœ“ Backup size trend
  â””â”€ Alert if suddenly 10x larger (possible runaway data)

âœ“ Storage usage
  â””â”€ Alert if backup storage > 80% full

âœ“ Upload success
  â””â”€ Alert if S3/storage upload fails

âœ“ Database integrity
  â””â”€ Test restore of DB backup weekly

âœ“ Git push status
  â””â”€ Alert if GitHub push fails (network issues, auth)
```

### Alert Thresholds

```
CRITICAL (notify immediately):
- Backup fails (exit code != 0)
- Storage full (< 100 MB free)
- Upload fails (S3 connection error)

WARNING (check next day):
- Backup takes > 10 minutes (slower than normal)
- Backup size grows 50% unexpectedly
- OpenClaw cron daemon not running

INFO (log only):
- Backup completed successfully
- Files backed up: X, Size: Y GB
- Upload to S3: OK
```

### Health Check Script

```bash
#!/bin/bash
# Run once per hour to verify backup system is healthy

# 1. Check if last backup completed in last 2.5 hours
LAST_BACKUP=$(stat -f %m /mnt/backups/latest.tar.gz.gpg)
CURRENT_TIME=$(date +%s)
DIFF=$((CURRENT_TIME - LAST_BACKUP))

if [ $DIFF -gt 9000 ]; then  # 2.5 hours in seconds
    ALERT="âŒ BACKUP FAILED: Last backup was $((DIFF/3600)) hours ago"
fi

# 2. Check storage space
FREE_SPACE=$(df /mnt/backups | awk 'NR==2 {print $4}')
if [ $FREE_SPACE -lt 100000 ]; then  # < 100 MB
    ALERT="âŒ STORAGE FULL: Only $((FREE_SPACE/1024)) MB available"
fi

# 3. Check database restore capability
if docker ps | grep -q postgres; then
    docker exec ai-erp-postgres pg_dump -U erp_user ai_erp > /dev/null
    if [ $? -ne 0 ]; then
        ALERT="âŒ DATABASE BACKUP FAILED: pg_dump error"
    fi
fi

if [ -z "$ALERT" ]; then
    echo "âœ… Backup system healthy at $(date)"
else
    echo "$ALERT"
    # Send alert to Telegram, email, etc.
fi
```

---

## ğŸš« PART 12: WHAT NOT TO DO (Critical Warnings)

### âŒ DON'T Backup While VS Code Is Actively Editing

**Problem**: Corrupted files, mid-write captures

**Solution**: 
- Use `--exclude` for active editor temp files
- Backup via `tar` (atomic), not `cp`
- Don't backup .vscode-server/ directly

### âŒ DON'T Store Backups on Same Disk as Source

**Problem**: Disk failure = source AND backup lost

**Solution**: External storage (USB, NAS, S3, etc.)

### âŒ DON'T Backup Unencrypted to Cloud

**Problem**: API keys exposed if S3 bucket breached

**Solution**: Encrypt with GPG before upload

### âŒ DON'T Use System Cron for This

**Problem**: Requires sudo, hard to integrate with OpenClaw, poor logging

**Solution**: Use OpenClaw's native cron system (what we're planning)

### âŒ DON'T Forget to Test Restores

**Problem**: Backup exists, but restore fails due to corruption

**Solution**: Monthly restore test on separate machine/VM

### âŒ DON'T Keep Passphrases in Code

**Problem**: Secrets exposed in git history

**Solution**: Store in environment variable or secure vault

---

## âœ… PART 13: IMPLEMENTATION CHECKLIST (Before Execution)

### Pre-Flight Checks (Non-Destructive)

```
â˜ Disk space available:
  â””â”€ Verify at least 5 GB external storage available

â˜ Git repos healthy:
  â””â”€ cd /home/ubuntu/.openclaw/workspace && git status
  â””â”€ cd /home/ubuntu/.openclaw/workspace/ai-erp && git status
  â””â”€ Expect: "On branch main" (or master), no conflicts

â˜ VS Code session verified:
  â””â”€ ps aux | grep vscode-server
  â””â”€ Expect: Multiple node processes running
  â””â”€ Action: Leave running - don't interrupt

â˜ OpenClaw cron system verified:
  â””â”€ cat /home/ubuntu/.openclaw/cron/jobs.json | grep -c '"id"'
  â””â”€ Expect: At least 4 existing jobs (currently all disabled)

â˜ Docker status (if using):
  â””â”€ docker ps
  â””â”€ Note: No containers currently running - that's OK

â˜ Sensitive file check:
  â””â”€ ls -la /home/ubuntu/.openclaw/openclaw.json
  â””â”€ Expect: File exists with API keys
  â””â”€ Action: These MUST be encrypted before upload

â˜ Backup location prepared:
  â””â”€ If USB: sudo mount /dev/sdb1 /mnt/backups && df -h /mnt/backups
  â””â”€ If S3: aws s3 ls s3://backup-bucket
  â””â”€ If NAS: mount -t nfs 192.168.1.x:/backups /mnt/backups
  â””â”€ Expect: Target directory exists and is writable

â˜ Test script created (before enabling cron):
  â””â”€ Create: /home/ubuntu/.openclaw/scripts/backup-all.sh
  â””â”€ Make executable: chmod +x /home/ubuntu/.openclaw/scripts/backup-all.sh
  â””â”€ Test manually: /home/ubuntu/.openclaw/scripts/backup-all.sh
  â””â”€ Expect: Backup file created successfully

â˜ Encryption key created (optional but recommended):
  â””â”€ gpg --gen-key (if using GPG)
  â””â”€ Or: Generate strong passphrase (20+ chars)
```

### First Run (Manual Backup Before Automation)

```
â˜ Run backup script manually:
  â””â”€ /home/ubuntu/.openclaw/scripts/backup-all.sh
  â””â”€ Monitor output
  â””â”€ Check: Backup file size is reasonable (~800MB - 2GB)
  â””â”€ Check: All components included (git, config, docs)

â˜ Verify backup contents:
  â””â”€ tar -tzf backup-2026-02-04.tar.gz | head -20
  â””â”€ Expect: openclaw.json, ai-erp/, workspace/ files

â˜ Test decrypt (if encrypted):
  â””â”€ gpg --decrypt backup-2026-02-04.tar.gz.gpg | tar -tzf - | head -20
  â””â”€ Expect: Same file listing as above

â˜ Upload to storage:
  â””â”€ If S3: aws s3 cp backup-2026-02-04.tar.gz.gpg s3://backups/
  â””â”€ If USB: cp backup-2026-02-04.tar.gz.gpg /mnt/backups/
  â””â”€ Expect: File appears on destination storage
```

### Enable Automation

```
â˜ Add job to /home/ubuntu/.openclaw/cron/jobs.json
  â””â”€ Start with "enabled": false (test first)
  â””â”€ Don't start with enabled: true yet

â˜ Verify OpenClaw reads updated jobs.json
  â””â”€ Restart OpenClaw daemon or wait for reload
  â””â”€ Check: jobs.json timestamp updated
  â””â”€ Note: No action needed if jobs.json has same content

â˜ Manual trigger first job:
  â””â”€ This is manual for now (don't enable recurring yet)
  â””â”€ Verify it runs and completes

â˜ Check execution log:
  â””â”€ cat /home/ubuntu/.openclaw/cron/runs/backup-every-2-hours.jsonl
  â””â”€ Expect: "status":"completed" with no errors

â˜ Enable recurring schedule:
  â””â”€ Edit jobs.json: "enabled": true
  â””â”€ Restart OpenClaw daemon or wait for reload
  â””â”€ Next backup will run automatically in 2 hours

â˜ Monitor first 48 hours:
  â””â”€ Check execution logs every 2 hours
  â””â”€ Verify backups appearing in storage
  â””â”€ Verify file sizes are consistent
  â””â”€ Set phone reminder to check logs
```

---

## ğŸ“ PART 14: QUESTIONS FOR GLENN

Before Glenn decides on implementation, clarify:

1. **Storage Location**
   - External USB drive (portable, limited size)?
   - AWS S3 (unlimited, need AWS account)?
   - Network NAS (local, need setup)?
   - GitHub (code only, free)?
   - Multiple tiers (gitâ†’GitHub, configâ†’S3)?

2. **Encryption Preference**
   - GPG (standard, requires passphrase)?
   - AWS KMS (cloud-native, if using S3)?
   - No encryption (for local storage only)?

3. **Retention Duration**
   - 7 days (standard, minimal storage)?
   - 30 days (comprehensive)?
   - 90 days (compliance)?

4. **Database Backup**
   - Include PostgreSQL backups (when Docker running)?
   - Include Redis snapshots?
   - Skip databases (just code & config)?

5. **Monitoring & Alerts**
   - Send alerts to Telegram?
   - Daily summary email?
   - Just log to file (manual check)?

6. **Disaster Recovery Testing**
   - Weekly restore tests?
   - Monthly only?
   - Not required?

---

## ğŸ¯ PART 15: EXECUTION ROADMAP (After Approval)

```
PHASE 1: Setup (Day 1)
â”œâ”€ â˜ Glenn approves storage location & answers Q.14
â”œâ”€ â˜ Provision storage (mount USB, create S3 bucket, etc.)
â”œâ”€ â˜ Create backup script: /home/ubuntu/.openclaw/scripts/backup-all.sh
â”œâ”€ â˜ Test manual backup (runs, completes, verifies)
â””â”€ Estimated time: 30 minutes

PHASE 2: Integration (Day 1-2)
â”œâ”€ â˜ Add job to cron/jobs.json
â”œâ”€ â˜ Manual trigger first scheduled backup
â”œâ”€ â˜ Verify logs in cron/runs/backup-every-2-hours.jsonl
â”œâ”€ â˜ Monitor for 24 hours (at least 12 backup cycles)
â””â”€ Estimated time: 1 hour + 24 hour monitoring

PHASE 3: Optimization (Day 3-7)
â”œâ”€ â˜ Monitor backup sizes & times
â”œâ”€ â˜ Adjust retention policy if needed
â”œâ”€ â˜ Create cleanup script for old backups
â”œâ”€ â˜ Setup health check alerts
â”œâ”€ â˜ Document restore procedure
â”œâ”€ â˜ Perform first restore test (on separate machine if possible)
â””â”€ Estimated time: 2 hours

PHASE 4: Production (Day 8+)
â”œâ”€ â˜ All systems running smoothly
â”œâ”€ â˜ Backups happening automatically every 2 hours
â”œâ”€ â˜ Zero manual intervention required
â”œâ”€ â˜ Monthly restore tests scheduled
â””â”€ Ongoing: Monitor health, rotate old backups

TOTAL IMPLEMENTATION TIME: ~4 hours + 24 hour testing
```

---

## ğŸ”„ DISASTER RECOVERY PROCEDURE

### If Everything Is Lost

```
STEP 1: Get one backup file
â””â”€ From USB: Plug in external drive
â””â”€ From S3: aws s3 cp s3://backups/2026-02-04_latest.tar.gz.gpg .

STEP 2: Decrypt (if encrypted)
â””â”€ gpg --decrypt 2026-02-04_latest.tar.gz.gpg > backup.tar.gz

STEP 3: Extract
â””â”€ mkdir -p /tmp/restore
â””â”€ cd /tmp/restore
â””â”€ tar -xzf backup.tar.gz

STEP 4: Restore components individually
â”œâ”€ Restore config:
â”‚  â””â”€ cp -r /tmp/restore/home/ubuntu/.openclaw /home/ubuntu/
â”œâ”€ Restore workspace:
â”‚  â””â”€ cp -r /tmp/restore/home/ubuntu/.openclaw/workspace /home/ubuntu/.openclaw/
â”œâ”€ Restore database (if needed):
â”‚  â””â”€ docker exec ai-erp-postgres pg_restore -U erp_user < backup.sql

STEP 5: Verify
â”œâ”€ cd /home/ubuntu/.openclaw && git status
â”œâ”€ cd /home/ubuntu/.openclaw/workspace/ai-erp && git status
â””â”€ Test: openclaw doctor (if command exists)

STEP 6: Restart services
â””â”€ docker-compose up -d (if using docker)
â””â”€ OpenClaw should auto-detect restored config
```

---

## ğŸ“Š SUMMARY TABLE

| Aspect | Details |
|--------|---------|
| **Frequency** | Every 2 hours (12 per day) |
| **Total Size** | ~8.5 MB code + config + docs (database when running: +50-200MB) |
| **Storage Needed (7d)** | 5-22 GB |
| **Storage Needed (30d)** | 22-93 GB |
| **Active Session** | VS Code (started 05:33 UTC) - SAFE TO BACKUP |
| **Automation Method** | OpenClaw's built-in cron (jobs.json) |
| **Backup Method** | tar + gpg encryption |
| **Storage Options** | S3 (recommended), External USB, NAS, GitHub |
| **Scheduling Type** | `"kind": "interval", "intervalMs": 7200000` |
| **Retention** | 7-day rolling + optional monthly archive |
| **Disk Space Available** | 1.7 GB free (76% full) - **CRITICAL** |
| **Implementation Risk** | LOW - Read-only operations, non-blocking |
| **Estimated Setup Time** | 30 minutes - 1 hour |
| **Testing Time** | 24 hours (let one full backup cycle run) |

---

## ğŸ¯ FINAL RECOMMENDATION

### Best Strategy for Glenn

**RECOMMENDED APPROACH:**

1. **Immediate** (This week)
   - Get external USB 1TB drive (~$50-100)
   - Mount to /mnt/backups
   - Create backup script
   - Run manual test backup

2. **Short-term** (Week 1-2)
   - Enable automatic backups every 2 hours
   - Monitor for 24-48 hours
   - Test restore procedure

3. **Medium-term** (Week 2-4)
   - Set up AWS S3 bucket for redundancy
   - Sync USB backups to S3 nightly
   - Schedule monthly restore tests

4. **Long-term** (Ongoing)
   - Monitor backup health automatically
   - Rotate old backups (keep 7 days locally, archive to S3)
   - Keep as part of operational routine

**Why this approach?**
- âœ… Solves immediate backup need (USB is cheap, fast)
- âœ… Doesn't disturb active VS Code session
- âœ… Uses OpenClaw's native cron system (no sudo needed)
- âœ… Can expand to S3 later without rewriting
- âœ… Safe, tested, reversible at each step
- âœ… Starts small, scales up gracefully

---

## ğŸ“ CONCLUSION

This plan provides:

âœ… **Complete backup** of ALL OpenClaw data, configuration, workspace, ERP project, and databases
âœ… **Automated execution** every 2 hours using OpenClaw's native cron system
âœ… **Safe for active processes** - read-only operations, non-blocking
âœ… **Multiple storage options** from cheap (USB) to enterprise (S3)
âœ… **Disaster recovery** - full restore possible from a single backup file
âœ… **Security** - optional GPG encryption for sensitive data
âœ… **Monitoring** - execution logs, health checks, alerts

**Status**: PLANNING COMPLETE - READY FOR APPROVAL & EXECUTION

---

**Next Step**: Glenn reviews this document and answers the questions in PART 14, then we proceed with PHASE 1 implementation.

